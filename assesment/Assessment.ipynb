{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f559330",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f841f7",
   "metadata": {},
   "source": [
    "# Assessment\n",
    "\n",
    "## Overview\n",
    "\n",
    "The goal of this assessment is to evaluate your ability to build and execute large models. Please demonstrate that ability by porting an existing piece of code into DeepSpeed and creating a series of configuration files to enable a range of DeepSpeed functions including: activation checkpointing, mixed precision training as well as ZeRo redundancy optimizer. \n",
    "\n",
    "To make the task containable, we have deliberately selected a simplified codebase, namely minGPT (https://github.com/karpathy/minGPT). This is a minimalistic implementation of Transformers that will not provide maximum performance, but which is representative and should allow you to complete this coding exercise in a relatively short period of time.\n",
    "\n",
    "In this task, we will look at yet another family of models, namely, Vision Transformers. Before diving into the assignment, please review the [code example](minGPT/minGPT/play_image.ipynb) that we will be using in this assessment. Feel free to execute the above code example but do bear in mind that training to convergence will take a considerable amount of time, so it  might help to finish it early and focus on the code migration discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f82105",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Conceptually, our goal will be to:\n",
    "- Migrate a standalone pytorch implementation of the training pipeline into DeepSpeed and train effectively on our \"two server\" cluster\n",
    "- Enable functionality that will allow for memory saving, namely: Mixed Precision Training, Activation Checkpointing and ZeRo Redundancy optimiser\n",
    "- Increase the size of the model being trained\n",
    "\n",
    "The below notebook will be a guide through the process and provide test code which will help determine whether you are on the right path to the correct solution. By the end of the assessment, when the code is complete, you will be asked to go back to the lab platform and press the `assess` button. This will trigger an automated process which will load your code files as well as the deepspeed configuration files and execute them, assessing correctness of the implementation. Please leave enough time to execute this step as it can take several minutes to compleate. If you are running out of time, please download the files you have modified so that they can be finished later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2dd24",
   "metadata": {},
   "source": [
    "### Hints\n",
    "* There are many different files in this assessment. To help keep track of what needs to be updated, we've placed `FIXME`s in relevant locations. If running a file results in an error, please look for a `FIXME`.\n",
    "* We will be processing a lot of data in this assessment. Please be patient with the hardware, and wait a minute between cancelling and running jobs.\n",
    "* If a clean slate is needed, please restart the server. Please download the following files and upload them in order to resume your work:\n",
    "    * [Assessment.ipynb](./Assessment.ipynb)\n",
    "    * [runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py)\n",
    "    * [trainer.py](./minGPT/minGPT/mingpt/trainer.py)\n",
    "    * [model.py](./minGPT/minGPT/mingpt/model.py)\n",
    "    * [runStep5.py](./minGPT/minGPT/runStep5.py)\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e977b",
   "metadata": {},
   "source": [
    "## Step 1: Baseline implementation\n",
    "\n",
    "Let us begin by looking at the starting point of our assessment, namely [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py). This is the same code that was reviewed earlier, just extracted into a python file to allow us for its batch execution. Let us test it to make sure it works in a standalone mode. Once again, training to convergence will take a substantial amount of time, so once you see training progress feel free to stop the training process and move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c92a28",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "170499072it [00:03, 49201915.09it/s]                                            \n",
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n",
      "50000 10000\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "04/15/2024 13:00:59 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "  0%|                                                 | 0/50000 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 459: train loss 6.07412. lr 2.760000e-05:   1%| | 460/50000 [00:20<^C\n",
      "epoch 1 iter 459: train loss 6.07412. lr 2.760000e-05:   1%| | 460/50000 [00:20<\n",
      "Traceback (most recent call last):\n",
      "  File \"minGPT/minGPT/runStartingPoint.py\", line 71, in <module>\n",
      "    trainer.train()\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 87, in train\n",
      "    run_epoch(train_loader, is_train=True)\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 57, in run_epoch\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/adamw.py\", line 137, in step\n",
      "    F.adamw(params_with_grad,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py\", line 139, in adamw\n",
      "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python minGPT/minGPT/runStartingPoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b336c",
   "metadata": {},
   "source": [
    "## Step 2: Enabling DeepSpeed\n",
    "\n",
    "Let's start by adapting the previous training scripts to use the DeepSpeed library by making some minimalistic changes in the code. To do so, you will need to:\n",
    "\n",
    "&nbsp; &nbsp; 1.  Modify the relevant sections in [runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py)   \n",
    "&nbsp; &nbsp; 2.  Modify the relevant sections in [trainer.py](./minGPT/minGPT/mingpt/trainer.py)   \n",
    "&nbsp; &nbsp; 3.  Create the DeepSpeed configuration file `ds_config_basic.json`   \n",
    "&nbsp; &nbsp; 4.  Run the training with `deepspeed` command\n",
    "\n",
    "\n",
    "### 1.  Modify the \"ToDo Step 2\" sections in the file `runFirstDeepSpeed.py`\n",
    "Open the file [runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py) and define the \"ToDo Step 2\" sections to port the code on DeepSpeed. There are 4 sections to be defined.\n",
    "\n",
    "### 2.  Modify the \"ToDo Step 2\" sections in the `trainer.py`\n",
    "Open the file [trainer.py](./minGPT/minGPT/mingpt/trainer.py) and implement the `DeepSpeedTrainer` class by defining the \"ToDo Step 2\" sections. There are 6 sections to be modified/implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7982e8e",
   "metadata": {},
   "source": [
    "### 3.  Create the DeepSpeed configuration file `ds_config_basic.json`\n",
    "In the next cell, change the `FIXME` to set:\n",
    "- The micro-batch size per gpu to 8\n",
    "- Make sure to enable Adam optimizer and copy the learning rate from the original code [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)\n",
    "- Set the gradient clipping to the value used in the original code [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac53079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/ds_config_basic.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/ds_config_basic.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 8,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-3\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298083e",
   "metadata": {},
   "source": [
    "### 4.  Run the training with `deepspeed` command\n",
    "\n",
    "The following command should result in 4 GPU training and we should see the training progress. Once again, the goal of this exercise is not to train this model to convergence. Once you see training taking place, you can interrupt the execution and move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1785d15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 13:54:07,105] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-04-15 13:54:07,424] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2024-04-15 13:54:08,406] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-04-15 13:54:09,534] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "04/15/2024 13:54:24 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "04/15/2024 13:54:25 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "04/15/2024 13:54:25 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "[2024-04-15 13:54:25,196] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "04/15/2024 13:54:25 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "04/15/2024 13:54:26 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "[2024-04-15 13:54:27,555] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10467195510864258 seconds\n",
      "[2024-04-15 13:54:28,100] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10186457633972168 seconds\n",
      "[2024-04-15 13:54:28,105] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-04-15 13:54:28,106] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-04-15 13:54:28,106] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2024-04-15 13:54:28,106] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-04-15 13:54:28,106] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "[2024-04-15 13:54:28,106] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "[2024-04-15 13:54:28,106] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-04-15 13:54:28,106] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2024-04-15 13:54:28,107] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   gradient_clipping ............ 1\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.003}\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   train_batch_size ............. 32\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2024-04-15 13:54:28,108] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2024-04-15 13:54:28,109] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2024-04-15 13:54:28,109] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2024-04-15 13:54:28,109] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.003\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1\n",
      "}\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10213375091552734 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "Loading extension module fused_adam...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Time to load fused_adam op: 0.10199427604675293 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.09686779975891113 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10148239135742188 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10146617889404297 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10144519805908203 seconds\n",
      "epoch 1 iter 9: train loss 6.27058. lr 4.800000e-06:   0%| | 10/12500 [00:01<25:[2024-04-15 13:54:30,438] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.32e-06], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 9: train loss 6.32213. lr 4.800000e-06:   0%| | 9/12500 [00:01<26:1[2024-04-15 13:54:30,439] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=288.4511536656477, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 18: train loss 6.22319. lr 9.120000e-06:   0%| | 19/12500 [00:03<23[2024-04-15 13:54:31,547] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.12e-06], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 19: train loss 6.20028. lr 9.600000e-06:   0%| | 20/12500 [00:03<22[2024-04-15 13:54:31,548] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=289.31361208643295, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 28: train loss 6.14306. lr 1.392000e-05:   0%| | 29/12500 [00:04<23[2024-04-15 13:54:32,657] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.392e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 29: train loss 6.17477. lr 1.440000e-05:   0%| | 30/12500 [00:04<23[2024-04-15 13:54:32,658] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=289.4406609490638, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 38: train loss 6.19640. lr 1.872000e-05:   0%| | 39/12500 [00:05<23[2024-04-15 13:54:33,776] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.872e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 39: train loss 6.09187. lr 1.920000e-05:   0%| | 40/12500 [00:05<23[2024-04-15 13:54:33,777] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=288.8879799538359, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 48: train loss 6.10995. lr 2.352000e-05:   0%| | 49/12500 [00:06<23[2024-04-15 13:54:34,890] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.352e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 49: train loss 6.06660. lr 2.400000e-05:   0%| | 50/12500 [00:06<23[2024-04-15 13:54:34,891] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=288.824077114307, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 59: train loss 5.96042. lr 2.880000e-05:   0%| | 59/12500 [00:07<23[2024-04-15 13:54:36,012] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=0, lr=[2.8320000000000003e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 59: train loss 6.10577. lr 2.880000e-05:   0%| | 59/12500 [00:07<23[2024-04-15 13:54:36,013] [INFO] [timer.py:193:stop] 0/60, SamplesPerSec=288.4372462483212, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 69: train loss 6.04493. lr 3.360000e-05:   1%| | 70/12500 [00:08<23[2024-04-15 13:54:37,135] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=0, lr=[3.312e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 69: train loss 6.17031. lr 3.360000e-05:   1%| | 69/12500 [00:08<23[2024-04-15 13:54:37,135] [INFO] [timer.py:193:stop] 0/70, SamplesPerSec=288.12920586211317, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 79: train loss 6.01412. lr 3.840000e-05:   1%| | 80/12500 [00:09<23[2024-04-15 13:54:38,256] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=0, lr=[3.792e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 79: train loss 5.96235. lr 3.840000e-05:   1%| | 80/12500 [00:09<23[2024-04-15 13:54:38,256] [INFO] [timer.py:193:stop] 0/80, SamplesPerSec=287.968293673239, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 89: train loss 5.86382. lr 4.320000e-05:   1%| | 89/12500 [00:10<23[2024-04-15 13:54:39,375] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=0, lr=[4.272e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 89: train loss 5.98932. lr 4.320000e-05:   1%| | 89/12500 [00:10<23[2024-04-15 13:54:39,375] [INFO] [timer.py:193:stop] 0/90, SamplesPerSec=287.8941706498272, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 99: train loss 5.93842. lr 4.800000e-05:   1%| | 99/12500 [00:11<23[2024-04-15 13:54:40,493] [INFO] [logging.py:69:log_dist] [Rank 0] step=100, skipped=0, lr=[4.752e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 99: train loss 5.97741. lr 4.800000e-05:   1%| | 99/12500 [00:12<23[2024-04-15 13:54:40,494] [INFO] [timer.py:193:stop] 0/100, SamplesPerSec=287.85681454915726, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 108: train loss 5.59213. lr 5.232000e-05:   1%| | 109/12500 [00:13<[2024-04-15 13:54:41,614] [INFO] [logging.py:69:log_dist] [Rank 0] step=110, skipped=0, lr=[5.232e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 109: train loss 5.82030. lr 5.280000e-05:   1%| | 110/12500 [00:13<[2024-04-15 13:54:41,615] [INFO] [timer.py:193:stop] 0/110, SamplesPerSec=287.7622394464489, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 118: train loss 5.85712. lr 5.712000e-05:   1%| | 119/12500 [00:14<[2024-04-15 13:54:42,740] [INFO] [logging.py:69:log_dist] [Rank 0] step=120, skipped=0, lr=[5.712e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 119: train loss 5.85629. lr 5.760000e-05:   1%| | 120/12500 [00:14<[2024-04-15 13:54:42,740] [INFO] [timer.py:193:stop] 0/120, SamplesPerSec=287.58959574372193, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 128: train loss 5.82240. lr 6.192000e-05:   1%| | 129/12500 [00:15<[2024-04-15 13:54:43,858] [INFO] [logging.py:69:log_dist] [Rank 0] step=130, skipped=0, lr=[6.192e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 129: train loss 5.91124. lr 6.240000e-05:   1%| | 130/12500 [00:15<[2024-04-15 13:54:43,859] [INFO] [timer.py:193:stop] 0/130, SamplesPerSec=287.58844180282165, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 138: train loss 5.66254. lr 6.672000e-05:   1%| | 139/12500 [00:16<[2024-04-15 13:54:44,977] [INFO] [logging.py:69:log_dist] [Rank 0] step=140, skipped=0, lr=[6.672e-05], mom=[(0.9, 0.999)]\n",
      "[2024-04-15 13:54:44,977] [INFO] [timer.py:193:stop] 0/140, SamplesPerSec=287.57065761943886, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 149: train loss 5.56951. lr 7.200000e-05:   1%| | 150/12500 [00:17<[2024-04-15 13:54:46,096] [INFO] [logging.py:69:log_dist] [Rank 0] step=150, skipped=0, lr=[7.152e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 149: train loss 5.42886. lr 7.200000e-05:   1%| | 149/12500 [00:17<[2024-04-15 13:54:46,096] [INFO] [timer.py:193:stop] 0/150, SamplesPerSec=287.5585129020524, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 158: train loss 5.62477. lr 7.632000e-05:   1%| | 159/12500 [00:18<[2024-04-15 13:54:47,212] [INFO] [logging.py:69:log_dist] [Rank 0] step=160, skipped=0, lr=[7.632e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 159: train loss 5.58915. lr 7.680000e-05:   1%| | 160/12500 [00:18<[2024-04-15 13:54:47,212] [INFO] [timer.py:193:stop] 0/160, SamplesPerSec=287.5937503042895, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 169: train loss 5.36470. lr 8.160000e-05:   1%| | 170/12500 [00:19<[2024-04-15 13:54:48,329] [INFO] [logging.py:69:log_dist] [Rank 0] step=170, skipped=0, lr=[8.112000000000001e-05], mom=[(0.9, 0.999)]\n",
      "[2024-04-15 13:54:48,329] [INFO] [timer.py:193:stop] 0/170, SamplesPerSec=287.60631363595235, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 178: train loss 5.14752. lr 8.592000e-05:   1%| | 179/12500 [00:20<[2024-04-15 13:54:49,454] [INFO] [logging.py:69:log_dist] [Rank 0] step=180, skipped=0, lr=[8.592e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 179: train loss 5.18637. lr 8.640000e-05:   1%| | 179/12500 [00:21<[2024-04-15 13:54:49,455] [INFO] [timer.py:193:stop] 0/180, SamplesPerSec=287.5007587000135, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 188: train loss 5.20439. lr 9.072000e-05:   2%| | 189/12500 [00:22<[2024-04-15 13:54:50,581] [INFO] [logging.py:69:log_dist] [Rank 0] step=190, skipped=0, lr=[9.072e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 189: train loss 5.22470. lr 9.120000e-05:   2%| | 190/12500 [00:22<[2024-04-15 13:54:50,581] [INFO] [timer.py:193:stop] 0/190, SamplesPerSec=287.39052427196145, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 198: train loss 5.17682. lr 9.552000e-05:   2%| | 199/12500 [00:23<[2024-04-15 13:54:51,706] [INFO] [logging.py:69:log_dist] [Rank 0] step=200, skipped=0, lr=[9.552000000000001e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 199: train loss 5.10117. lr 9.600000e-05:   2%| | 200/12500 [00:23<[2024-04-15 13:54:51,707] [INFO] [timer.py:193:stop] 0/200, SamplesPerSec=287.3025910348406, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 208: train loss 4.85201. lr 1.003200e-04:   2%| | 209/12500 [00:24<[2024-04-15 13:54:52,832] [INFO] [logging.py:69:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00010031999999999999], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 209: train loss 4.96612. lr 1.008000e-04:   2%| | 210/12500 [00:24<[2024-04-15 13:54:52,832] [INFO] [timer.py:193:stop] 0/210, SamplesPerSec=287.220620831109, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 216: train loss 4.99893. lr 1.041600e-04:   2%| | 217/12500 [00:25<^C\n",
      "[2024-04-15 13:54:53,701] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 12606\n",
      "[2024-04-15 13:54:53,701] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 12607\n",
      "[2024-04-15 13:54:53,702] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 12608\n",
      "[2024-04-15 13:54:53,703] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 12609\n",
      "[2024-04-15 13:54:53,703] [INFO] [launch.py:187:sigkill_handler] Main process received SIGINT, exiting\n"
     ]
    }
   ],
   "source": [
    "!deepspeed minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c096d",
   "metadata": {},
   "source": [
    "## Step 3: Multi node execution\n",
    "\n",
    "The above code executed on 4 GPUs for this particular node, but our goal is to make it work across the two nodes we have used earlier in the class. Please reuse the code we have worked with earlier to launch a `2` node job executing the above. Let us start by creating the appropriate shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8bdff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep3.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep3.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step3\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=1\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runFirstDeepSpeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config minGPT/minGPT/ds_config_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaf791",
   "metadata": {},
   "source": [
    "Please modify the below to enable multi-node execution. Please use the below command to execute your multi-node job (this is the command that will be used for assessment so do not change the file names or paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0604d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 2  slurmpar dli_asse    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "!sbatch ./minGPT/minGPT/runSlurmStep3.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676733e2",
   "metadata": {},
   "source": [
    "Once the above executes, we should be able to see output and error logs with the commands below. Make sure to copy the job ID to the below command. Once again, make sure the code deploys logs out the below location with the below file name structure as those will be inspected for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a131acc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 13:56:40,121] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-04-15 13:56:40,122] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-04-15 13:56:40,122] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzBdLCAic2x1cm1ub2RlMiI6IFswXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config 'minGPT/minGPT/ds_config_basic.json'\n",
      "slurmnode1: [2024-04-15 13:56:41,472] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-04-15 13:56:41,472] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0], 'slurmnode2': [0]}\n",
      "slurmnode1: [2024-04-15 13:56:41,472] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=1, node_rank=0\n",
      "slurmnode1: [2024-04-15 13:56:41,473] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0], 'slurmnode2': [1]})\n",
      "slurmnode1: [2024-04-15 13:56:41,473] [INFO] [launch.py:123:main] dist_world_size=2\n",
      "slurmnode1: [2024-04-15 13:56:41,473] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0], 'slurmnode2': [0]}\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=1, node_rank=1\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0], 'slurmnode2': [1]})\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:123:main] dist_world_size=2\n",
      "slurmnode2: [2024-04-15 13:56:41,485] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "slurmnode1: [2024-04-15 13:56:42,549] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode2: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode1: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode2: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode1: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-04-15 13:56:59,697] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode1: [2024-04-15 13:57:01,526] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Detected CUDA files, patching ldflags\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode2: Building extension module fused_adam...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.04425358772277832 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode2: Building extension module utils...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.04172801971435547 seconds\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.10214710235595703 seconds\n",
      "slurmnode1: [2024-04-15 13:57:02,095] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode1: [2024-04-15 13:57:02,100] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "slurmnode1: [2024-04-15 13:57:02,100] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-04-15 13:57:02,101] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-04-15 13:57:02,101] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-04-15 13:57:02,101] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:02,101] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-04-15 13:57:02,101] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": false, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": false, \n",
      "slurmnode1:     \"cpu_checkpointing\": false, \n",
      "slurmnode1:     \"number_checkpoints\": null, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": false, \n",
      "slurmnode1:     \"profile\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   gradient_clipping ............ 1\n",
      "slurmnode1: [2024-04-15 13:57:02,102] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.003}\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   train_batch_size ............. 16\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   world_size ................... 2\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 0, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": false, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": null, \n",
      "slurmnode1:     \"offload_optimizer\": null, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "slurmnode1: [2024-04-15 13:57:02,103] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "slurmnode1: [2024-04-15 13:57:02,192] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 8, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode1: Building extension module utils...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.04957914352416992 seconds\n",
      "slurmnode1: [2024-04-15 13:57:06,566] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.32e-06], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:06,567] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=50.12721667858058, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:09,763] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.12e-06], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:09,764] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=50.153700580602504, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:12,907] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.392e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:12,908] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=50.45929921430522, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:16,103] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.872e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:16,104] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=50.38477443541802, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:19,239] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.352e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:19,240] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=50.542569249024986, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:22,397] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=0, lr=[2.8320000000000003e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:22,397] [INFO] [timer.py:193:stop] 0/60, SamplesPerSec=50.58403091729259, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:25,624] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=0, lr=[3.312e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:25,625] [INFO] [timer.py:193:stop] 0/70, SamplesPerSec=50.45290841206162, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:28,851] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=0, lr=[3.792e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:28,851] [INFO] [timer.py:193:stop] 0/80, SamplesPerSec=50.35711950260995, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:31,987] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=0, lr=[4.272e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:31,987] [INFO] [timer.py:193:stop] 0/90, SamplesPerSec=50.4451488684035, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:35,124] [INFO] [logging.py:69:log_dist] [Rank 0] step=100, skipped=0, lr=[4.752e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:35,124] [INFO] [timer.py:193:stop] 0/100, SamplesPerSec=50.51440783892839, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:38,270] [INFO] [logging.py:69:log_dist] [Rank 0] step=110, skipped=0, lr=[5.232e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:38,271] [INFO] [timer.py:193:stop] 0/110, SamplesPerSec=50.55630653390365, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:41,407] [INFO] [logging.py:69:log_dist] [Rank 0] step=120, skipped=0, lr=[5.712e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:41,407] [INFO] [timer.py:193:stop] 0/120, SamplesPerSec=50.604283078006254, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:44,550] [INFO] [logging.py:69:log_dist] [Rank 0] step=130, skipped=0, lr=[6.192e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:44,551] [INFO] [timer.py:193:stop] 0/130, SamplesPerSec=50.63657582232701, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:47,688] [INFO] [logging.py:69:log_dist] [Rank 0] step=140, skipped=0, lr=[6.672e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:47,688] [INFO] [timer.py:193:stop] 0/140, SamplesPerSec=50.671296876802764, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:50,833] [INFO] [logging.py:69:log_dist] [Rank 0] step=150, skipped=0, lr=[7.152e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:50,833] [INFO] [timer.py:193:stop] 0/150, SamplesPerSec=50.693056525413425, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:53,967] [INFO] [logging.py:69:log_dist] [Rank 0] step=160, skipped=0, lr=[7.632e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:53,967] [INFO] [timer.py:193:stop] 0/160, SamplesPerSec=50.72331408911977, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:57:57,110] [INFO] [logging.py:69:log_dist] [Rank 0] step=170, skipped=0, lr=[8.112000000000001e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:57:57,111] [INFO] [timer.py:193:stop] 0/170, SamplesPerSec=50.74146780743018, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:00,263] [INFO] [logging.py:69:log_dist] [Rank 0] step=180, skipped=0, lr=[8.592e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:00,263] [INFO] [timer.py:193:stop] 0/180, SamplesPerSec=50.74912081681366, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:03,405] [INFO] [logging.py:69:log_dist] [Rank 0] step=190, skipped=0, lr=[9.072e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:03,405] [INFO] [timer.py:193:stop] 0/190, SamplesPerSec=50.76507303788236, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:06,555] [INFO] [logging.py:69:log_dist] [Rank 0] step=200, skipped=0, lr=[9.552000000000001e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:06,555] [INFO] [timer.py:193:stop] 0/200, SamplesPerSec=50.77288533662261, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:09,693] [INFO] [logging.py:69:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00010031999999999999], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:09,693] [INFO] [timer.py:193:stop] 0/210, SamplesPerSec=50.78902323789746, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:12,837] [INFO] [logging.py:69:log_dist] [Rank 0] step=220, skipped=0, lr=[0.00010512000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:12,837] [INFO] [timer.py:193:stop] 0/220, SamplesPerSec=50.799080692739494, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:15,989] [INFO] [logging.py:69:log_dist] [Rank 0] step=230, skipped=0, lr=[0.00010992], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:15,990] [INFO] [timer.py:193:stop] 0/230, SamplesPerSec=50.80268278755819, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:19,134] [INFO] [logging.py:69:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00011472000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:19,134] [INFO] [timer.py:193:stop] 0/240, SamplesPerSec=50.81127289779449, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:22,273] [INFO] [logging.py:69:log_dist] [Rank 0] step=250, skipped=0, lr=[0.00011952000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:22,274] [INFO] [timer.py:193:stop] 0/250, SamplesPerSec=50.82294650745839, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:25,416] [INFO] [logging.py:69:log_dist] [Rank 0] step=260, skipped=0, lr=[0.00012432], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:25,416] [INFO] [timer.py:193:stop] 0/260, SamplesPerSec=50.83142642653584, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:28,566] [INFO] [logging.py:69:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00012912], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:28,567] [INFO] [timer.py:193:stop] 0/270, SamplesPerSec=50.834198011256696, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:31,728] [INFO] [logging.py:69:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00013392], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:31,728] [INFO] [timer.py:193:stop] 0/280, SamplesPerSec=50.83063229583004, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:34,880] [INFO] [logging.py:69:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00013872], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:34,880] [INFO] [timer.py:193:stop] 0/290, SamplesPerSec=50.83258646012207, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:38,029] [INFO] [logging.py:69:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00014352], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:38,029] [INFO] [timer.py:193:stop] 0/300, SamplesPerSec=50.83588789257979, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:41,169] [INFO] [logging.py:69:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00014832], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:41,169] [INFO] [timer.py:193:stop] 0/310, SamplesPerSec=50.84399247095277, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:44,316] [INFO] [logging.py:69:log_dist] [Rank 0] step=320, skipped=0, lr=[0.00015312], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:44,317] [INFO] [timer.py:193:stop] 0/320, SamplesPerSec=50.84779727438548, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:47,461] [INFO] [logging.py:69:log_dist] [Rank 0] step=330, skipped=0, lr=[0.00015792], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:47,462] [INFO] [timer.py:193:stop] 0/330, SamplesPerSec=50.85240687881511, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:50,602] [INFO] [logging.py:69:log_dist] [Rank 0] step=340, skipped=0, lr=[0.00016272], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:50,603] [INFO] [timer.py:193:stop] 0/340, SamplesPerSec=50.85875813795935, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:53,718] [INFO] [logging.py:69:log_dist] [Rank 0] step=350, skipped=0, lr=[0.00016752], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:53,718] [INFO] [timer.py:193:stop] 0/350, SamplesPerSec=50.87672843967242, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:56,820] [INFO] [logging.py:69:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00017232], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:56,820] [INFO] [timer.py:193:stop] 0/360, SamplesPerSec=50.89949973818667, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:58:59,935] [INFO] [logging.py:69:log_dist] [Rank 0] step=370, skipped=0, lr=[0.00017712], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:58:59,936] [INFO] [timer.py:193:stop] 0/370, SamplesPerSec=50.9155363017132, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:03,049] [INFO] [logging.py:69:log_dist] [Rank 0] step=380, skipped=0, lr=[0.00018192], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:03,049] [INFO] [timer.py:193:stop] 0/380, SamplesPerSec=50.931426482647, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:06,185] [INFO] [logging.py:69:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00018672], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:06,186] [INFO] [timer.py:193:stop] 0/390, SamplesPerSec=50.93679587527563, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:09,301] [INFO] [logging.py:69:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00019151999999999998], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:09,302] [INFO] [timer.py:193:stop] 0/400, SamplesPerSec=50.950314757257324, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:12,429] [INFO] [logging.py:69:log_dist] [Rank 0] step=410, skipped=0, lr=[0.00019632], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:12,429] [INFO] [timer.py:193:stop] 0/410, SamplesPerSec=50.95825515409645, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:15,534] [INFO] [logging.py:69:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00020112], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:15,534] [INFO] [timer.py:193:stop] 0/420, SamplesPerSec=50.975171121100296, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-04-15 13:59:18,644] [INFO] [logging.py:69:log_dist] [Rank 0] step=430, skipped=0, lr=[0.00020592000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 13:59:18,644] [INFO] [timer.py:193:stop] 0/430, SamplesPerSec=50.9887239903525, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=2;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad5e8de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode2: Warning: Permanently added 'slurmnode2,172.18.0.2' (ECDSA) to the list of known hosts.\n",
      "170499072it [00:04, 40476475.99it/s]                               \n",
      "170499072it [00:05, 33314761.37it/s]                               \n",
      "slurmnode2: 04/15/2024 13:56:59 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 04/15/2024 13:56:59 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 04/15/2024 13:57:00 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode1: 04/15/2024 13:57:01 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode1: 04/15/2024 13:57:01 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "slurmnode2: 04/15/2024 13:57:01 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=2;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc375e07",
   "metadata": {},
   "source": [
    "Once you are happy with your code, please make sure the batch job is terminated before going to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2808a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 2  slurmpar dli_asse    admin  R       2:38      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "327e3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel  2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb8cd8",
   "metadata": {},
   "source": [
    "## Step 4: Further code improvement\n",
    "\n",
    "We are missing capability to do activation checkpointing. In this step, we will introduce code that will allow us to do activation checkpointing with DeepSpeed library.\n",
    "\n",
    "&nbsp; &nbsp; 1. Define the transformer blocks for activation checkpointing   \n",
    "&nbsp; &nbsp; 2. Create the DeepSeed configuration file enabeling activation checkpointing and FP16 training   \n",
    "&nbsp; &nbsp; 3. Create and run the sbatch training file  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08ad86",
   "metadata": {},
   "source": [
    "### 1. Define the transformer blocks for activation checkpointing\n",
    "\n",
    "To enable activation checkpointing of a model (or part of the model) with DeepSpeed, at the forward pass definition, we need to wrap each block with the function `deepspeed.checkpointing.checkpoint()` ([learn more](https://deepspeed.readthedocs.io/en/stable/activation-checkpointing.html#deepspeed.checkpointing.checkpoint)). \n",
    "\n",
    "The example bellow shows a simple convolutional Network definition with 2 CNN blocks followed by a linear layer in which the CNN blocks are wrapped for activation checkpointing with DeepSpeed.\n",
    "\n",
    "```\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[nn.Conv2d(3, 32, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.cnn_block_2 = nn.Sequential(*[nn.Conv2d(64, 64, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.linearize = nn.Sequential(*[ nn.Linear(64 * 8 * 8, 512),nn.ReLU()])\n",
    "        self.out = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_1, X)\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_2, X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.linearize(X)\n",
    "        X = self.out(X)\n",
    "        return X\n",
    "\n",
    "```\n",
    "A similar mechanism is implemented with torch via the function `torch.utils.checkpoint.checkpoint()`.\n",
    "\n",
    "\n",
    "In our case, the VisionTransformer model is implemented as the GPT class in the file `./minGPT/minGPT/mingpt/model.py`. \n",
    "\n",
    "**TODO:** Modify the \"Step 4 ToDo\" task in [model.py](./minGPT/minGPT/mingpt/model.py) file in order to make the transformer blocks wrapped by the DeepSpeed activation checkpointing. Replace `x = self.blocks(x)` with:\n",
    "```\n",
    "for block in self.blocks:\n",
    "    x = deepspeed.checkpointing.checkpoint(block, x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193f640",
   "metadata": {},
   "source": [
    "### 2. Create the DeepSeed configuration file\n",
    "\n",
    "Before starting, you can check the DeepSpeed documentation of the config-json file for the [activation-checkpointing.](https://www.deepspeed.ai/docs/config-json/#activation-checkpointing)\n",
    "\n",
    "Create the `ds_config_step4.json` by modifying the `#FIXME` in the cell bellow to:\n",
    "- Enable activation checkpointing\n",
    "- Make the micro batch size per GPU to 128 to make sure activation checkpointing is working well\n",
    "- Make the number of checkpoints to 12\n",
    "- Enable FP16 training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4045c401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing minGPT/minGPT/ds_config_step4.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step4.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"cpu_checkpointing\": true,\n",
    "    \"contiguous_memory_optimization\": true,\n",
    "    \"number_checkpoints\": 12,\n",
    "    \"synchronize_checkpoint_boundary\": true,\n",
    "    \"profile\": true\n",
    "    },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648897d",
   "metadata": {},
   "source": [
    "### 3. Run the sbatch training file\n",
    "\n",
    "\n",
    "Let's start by creating copies of the training python scripts `runFirstDeepSpeed.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abb98fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c95d1",
   "metadata": {},
   "source": [
    "Let's now create the sbatch file `runSlurmStep4.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cd3cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./minGPT/minGPT/runSlurmStep4.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep4.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step4\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep4.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb4a5c",
   "metadata": {},
   "source": [
    "Once you have done the above, please run the with the below command to submit the training job to the slurm scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93876a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 3  slurmpar dli_asse    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep4.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56370d5a",
   "metadata": {},
   "source": [
    "Verify the execution of your code using the below (you should see it progress despite the large batch size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a572e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 14:05:06,629] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-04-15 14:05:06,630] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-04-15 14:05:06,630] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runStep4.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step4.json'\n",
      "slurmnode2: [2024-04-15 14:05:07,987] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-04-15 14:05:07,987] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2024-04-15 14:05:07,987] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2024-04-15 14:05:07,987] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2024-04-15 14:05:07,988] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2024-04-15 14:05:07,988] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2024-04-15 14:05:07,994] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-04-15 14:05:09,112] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-04-15 14:05:26,894] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-04-15 14:05:30,590] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Detected CUDA files, patching ldflags\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode2: Building extension module fused_adam...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.05399179458618164 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode2: Building extension module utils...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.04722285270690918 seconds\n",
      "slurmnode2: Detected CUDA files, patching ldflags\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode2: Building extension module fused_adam...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.0444798469543457 seconds\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.10200667381286621 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode2: Building extension module utils...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.06660079956054688 seconds\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.202040433883667 seconds\n",
      "slurmnode1: [2024-04-15 14:05:31,256] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode1: [2024-04-15 14:05:31,262] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "slurmnode1: [2024-04-15 14:05:31,262] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "slurmnode1: [2024-04-15 14:05:31,271] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-04-15 14:05:31,271] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-04-15 14:05:31,271] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-04-15 14:05:31,271] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 14:05:31,271] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": true, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": true, \n",
      "slurmnode1:     \"cpu_checkpointing\": true, \n",
      "slurmnode1:     \"number_checkpoints\": 12, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": true, \n",
      "slurmnode1:     \"profile\": true\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:05:31,272] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   train_batch_size ............. 512\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  128\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 0, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": false, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": null, \n",
      "slurmnode1:     \"offload_optimizer\": null, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "slurmnode1: [2024-04-15 14:05:31,273] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "slurmnode1: [2024-04-15 14:05:31,275] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 128, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.0003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0, \n",
      "slurmnode1:     \"activation_checkpointing\": {\n",
      "slurmnode1:         \"partition_activations\": true, \n",
      "slurmnode1:         \"cpu_checkpointing\": true, \n",
      "slurmnode1:         \"contiguous_memory_optimization\": true, \n",
      "slurmnode1:         \"number_checkpoints\": 12, \n",
      "slurmnode1:         \"synchronize_checkpoint_boundary\": true, \n",
      "slurmnode1:         \"profile\": true\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"fp16\": {\n",
      "slurmnode1:         \"enabled\": true\n",
      "slurmnode1:     }\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode1: Building extension module utils...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.05828118324279785 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.20304346084594727 seconds\n",
      "slurmnode1: [2024-04-15 14:05:33,090] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information\n",
      "slurmnode1: [2024-04-15 14:05:33,090] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False\n",
      "slurmnode1: [2024-04-15 14:05:33,090] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with None total layers\n",
      "slurmnode1: [2024-04-15 14:05:33,090] [INFO] [checkpointing.py:554:forward] ----Synchronization False\n",
      "slurmnode1: [2024-04-15 14:05:33,090] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False\n",
      "slurmnode2: [2024-04-15 14:05:41,304] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 0\n",
      "slurmnode2: [2024-04-15 14:05:41,304] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-04-15 14:05:41,304] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 0\n",
      "slurmnode1: [2024-04-15 14:05:41,304] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-04-15 14:05:41,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 0\n",
      "slurmnode1: [2024-04-15 14:05:41,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-04-15 14:05:41,305] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
      "slurmnode2: [2024-04-15 14:05:41,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 0\n",
      "slurmnode2: [2024-04-15 14:05:41,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode2: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 1\n",
      "slurmnode2: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 1\n",
      "slurmnode2: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode2: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 1\n",
      "slurmnode1: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 1\n",
      "slurmnode1: [2024-04-15 14:05:46,013] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-04-15 14:05:46,013] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
      "slurmnode2: [2024-04-15 14:05:50,726] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 2\n",
      "slurmnode2: [2024-04-15 14:05:50,726] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode1: [2024-04-15 14:05:50,726] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 2\n",
      "slurmnode1: [2024-04-15 14:05:50,726] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode1: [2024-04-15 14:05:50,727] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 2\n",
      "slurmnode1: [2024-04-15 14:05:50,727] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode1: [2024-04-15 14:05:50,727] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
      "slurmnode2: [2024-04-15 14:05:50,727] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 2\n",
      "slurmnode2: [2024-04-15 14:05:50,727] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode2: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 3\n",
      "slurmnode2: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode2: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 3\n",
      "slurmnode2: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode1: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 3\n",
      "slurmnode1: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode1: [2024-04-15 14:05:55,444] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n",
      "slurmnode1: [2024-04-15 14:05:55,443] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 3\n",
      "slurmnode1: [2024-04-15 14:05:55,444] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode1: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 4\n",
      "slurmnode1: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 4\n",
      "slurmnode2: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 4\n",
      "slurmnode2: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-04-15 14:06:00,167] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-04-15 14:06:00,168] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n",
      "slurmnode2: [2024-04-15 14:06:00,168] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 4\n",
      "slurmnode2: [2024-04-15 14:06:00,169] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode2: [2024-04-15 14:06:04,895] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 5\n",
      "slurmnode2: [2024-04-15 14:06:04,895] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode1: [2024-04-15 14:06:04,895] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 5\n",
      "slurmnode1: [2024-04-15 14:06:04,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode1: [2024-04-15 14:06:04,896] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n",
      "slurmnode1: [2024-04-15 14:06:04,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 5\n",
      "slurmnode1: [2024-04-15 14:06:04,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode2: [2024-04-15 14:06:04,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 5\n",
      "slurmnode2: [2024-04-15 14:06:04,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode2: [2024-04-15 14:06:09,609] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 6\n",
      "slurmnode2: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 6\n",
      "slurmnode1: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 6\n",
      "slurmnode1: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-04-15 14:06:09,610] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n",
      "slurmnode2: [2024-04-15 14:06:09,610] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 6\n",
      "slurmnode2: [2024-04-15 14:06:09,611] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 7\n",
      "slurmnode1: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode1: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 7\n",
      "slurmnode1: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode1: [2024-04-15 14:06:14,330] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n",
      "slurmnode2: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 7\n",
      "slurmnode2: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode2: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 7\n",
      "slurmnode2: [2024-04-15 14:06:14,330] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode2: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 8\n",
      "slurmnode2: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 8\n",
      "slurmnode1: Grad overflow on iteration 8\n",
      "slurmnode1: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode1: [2024-04-15 14:06:19,048] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n",
      "slurmnode2: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode2: [2024-04-15 14:06:19,048] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode1: [2024-04-15 14:06:19,049] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 8\n",
      "slurmnode1: [2024-04-15 14:06:19,049] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 9\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 9\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode2: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 9\n",
      "slurmnode2: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode2: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 9\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n",
      "slurmnode2: [2024-04-15 14:06:23,768] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode1: [2024-04-15 14:06:23,768] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=10, lr=[6.912e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 14:06:23,769] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=108.50577214365933, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n",
      "slurmnode1: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 10\n",
      "slurmnode1: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 10\n",
      "slurmnode1: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-04-15 14:06:28,499] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n",
      "slurmnode2: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 10\n",
      "slurmnode2: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode2: [2024-04-15 14:06:28,499] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 10\n",
      "slurmnode2: [2024-04-15 14:06:28,500] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 11\n",
      "slurmnode1: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode2: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 11\n",
      "slurmnode2: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode2: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 11\n",
      "slurmnode1: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode2: Grad overflow on iteration 11\n",
      "slurmnode2: [2024-04-15 14:06:33,234] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode1: [2024-04-15 14:06:33,234] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n",
      "slurmnode2: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 12\n",
      "slurmnode2: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode1: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 12\n",
      "slurmnode1: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode2: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 12\n",
      "slurmnode1: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 12\n",
      "slurmnode1: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode2: [2024-04-15 14:06:37,964] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode1: [2024-04-15 14:06:37,964] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
      "slurmnode1: [2024-04-15 14:06:42,695] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 13\n",
      "slurmnode1: [2024-04-15 14:06:42,695] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode2: [2024-04-15 14:06:42,695] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 13\n",
      "slurmnode2: [2024-04-15 14:06:42,695] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 13\n",
      "slurmnode2: [2024-04-15 14:06:42,696] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode2: [2024-04-15 14:06:42,696] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode1: [2024-04-15 14:06:42,696] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 13\n",
      "slurmnode1: [2024-04-15 14:06:42,696] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode1: [2024-04-15 14:06:42,696] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n",
      "slurmnode1: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 14\n",
      "slurmnode1: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode2: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 14\n",
      "slurmnode2: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode1: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 14\n",
      "slurmnode2: [2024-04-15 14:06:47,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode1: Grad overflow on iteration 14\n",
      "slurmnode1: [2024-04-15 14:06:47,439] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode1: [2024-04-15 14:06:47,439] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=3;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad4ddc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: 04/15/2024 14:05:26 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 04/15/2024 14:05:27 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 04/15/2024 14:05:28 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 04/15/2024 14:05:28 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 04/15/2024 14:05:29 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode1: 04/15/2024 14:05:29 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode2: 04/15/2024 14:05:29 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode2: 04/15/2024 14:05:30 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode2: 04/15/2024 14:05:30 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 04/15/2024 14:05:30 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 04/15/2024 14:05:30 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 04/15/2024 14:05:30 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=3;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf31628",
   "metadata": {},
   "source": [
    "Don't forget to cancel execution of your batch job once you are happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4de9ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel  3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3975d9",
   "metadata": {},
   "source": [
    "### Further optimization consideration \n",
    "All workers participating in the training process are generating the same output. Thus, the k-means is computed twice. \n",
    "It is possible to ajust the k-means implmentation to execute it just once and with a redistribution of the results across all of the workers. \n",
    "Bellow an example on how to do it:\n",
    "\n",
    "```import torch.distributed as dist\n",
    "def run_kmeans(x, ncluster, niter=8, rank, size):\n",
    "    print('KMeans executed on rank ', rank, ' Worlds size ', size)\n",
    "    N, D = x.size()\n",
    "    c = x[torch.randperm(N)[:ncluster]] # init clusters at random\n",
    "    c = c.cuda(args.local_rank) # move the tensor to the GPU for exchange\n",
    "    if rank == 0:\n",
    "        # Computing KMeans only on rank 0 \n",
    "        with torch.no_grad():\n",
    "            c = kmeans(x, ncluster, niter)\n",
    "    # We now have computed the clusters so can proceed to the exchange\n",
    "    dist.barrier()\n",
    "    print('Broadcasting')\n",
    "    dist.broadcast(C.cuda(args.local_rank), src=0)\n",
    "    c=c.cpu()\n",
    "    print('Rank ', rank, ' has data ', C.size())\n",
    "    return c\n",
    "\n",
    "C=run_kmeans(px, ncluster, niter=8, dist.get_rank(), dist.get_world_size())    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f7c08",
   "metadata": {},
   "source": [
    "## Step 5: Scaling up\n",
    "\n",
    "Now that we have a minimal functional implemented, let's scale out the training job. In this part of the assessment, we will make the model substantially bigger. \n",
    "\n",
    "\n",
    "&nbsp; &nbsp; 1. Scale the model's architecture   \n",
    "&nbsp; &nbsp; 2. Create the DeepSeed configuration file enabeling activation checkpointing, FP16 training, ZeRO optimizer     \n",
    "&nbsp; &nbsp; 3. Create and run the sbatch training file  \n",
    "\n",
    "### 1. Scale the model's architecture\n",
    "Before modifying the training script, let's start by making a copy to be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ecf2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26260e",
   "metadata": {},
   "source": [
    "**TODO**: Adjust the number of layers of the VisionTransformers to **24** by modifying the [runStep5.py](./minGPT/minGPT/runStep5.py) on the \"GPTConfig\" section where the architecture of the neural network dimensions is defined as: \n",
    "```\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0,\n",
    "                  n_layer=12, n_head=8, n_embd=256)\n",
    "```\n",
    "\n",
    "\n",
    "### 2. Create the DeepSeed configuration file enabling activation checkpointing, FP16 training, ZeRO optimizer\n",
    "\n",
    "Alter [ds_config_step5.json](./minGPT/minGPT/ds_config_step5.json) to reconfigure be enabling:\n",
    "- Gradient accumulation and execute 4 accumulation steps to increase the global batch size (which is frequently needed to maintain fixed hyperparameters).\n",
    "- Activation checkpointing to create 24 rather than 12 checkpoints\n",
    "- FP16 training\n",
    "- ZeRo Stage 3 optimizer with CPU offload for both parameters and optimizer states. Check the [ZeRO documentation](https://deepspeed.readthedocs.io/en/latest/zero3.html) for more details. \n",
    "\n",
    "**Hint:** Notebook 6 in Lab 1 has information on the ZeRo Stage 3 optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd6ceec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing minGPT/minGPT/ds_config_step5.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step5.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"gradient_accumulation_steps\": 4,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"cpu_checkpointing\": true,\n",
    "    \"contiguous_memory_optimization\": true,\n",
    "    \"number_checkpoints\": 24,\n",
    "    \"synchronize_checkpoint_boundary\": true,\n",
    "    \"profile\": true\n",
    "    },\n",
    "   \"fp16\": {\n",
    "    \"enabled\": true\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_prefetch_bucket_size\": 5e8,\n",
    "    \"stage3_param_persitence_threshold\": 1e6,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb300e",
   "metadata": {},
   "source": [
    "### 3. Create and run the sbatch training file \n",
    "Execute the next cell to generate the sbatch script for the step5 training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef381729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./minGPT/minGPT/runSlurmStep5.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep5.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step5\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep5.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f664617",
   "metadata": {},
   "source": [
    "Once you have made the above changes please execute your job with the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "207a33f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 4  slurmpar dli_asse    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep5.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea58a20",
   "metadata": {},
   "source": [
    "Verify the execution of your code using the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ae5120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 14:18:30,265] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-04-15 14:18:30,265] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-04-15 14:18:30,265] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runStep5.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step5.json'\n",
      "slurmnode2: [2024-04-15 14:18:31,564] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-04-15 14:18:31,565] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2024-04-15 14:18:31,565] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2024-04-15 14:18:31,565] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2024-04-15 14:18:31,565] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2024-04-15 14:18:31,565] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-04-15 14:18:31,615] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-04-15 14:18:31,616] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2024-04-15 14:18:31,616] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2024-04-15 14:18:31,616] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2024-04-15 14:18:31,616] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2024-04-15 14:18:31,616] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-04-15 14:18:32,738] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode1: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verified\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-04-15 14:18:50,968] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode1: [2024-04-15 14:18:53,348] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Creating extension directory /home/admin/.cache/torch_extensions/py38_cu115/cpu_adam...\n",
      "slurmnode1: Detected CUDA files, patching ldflags\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/cpu_adam/build.ninja...\n",
      "slurmnode1: Building extension module cpu_adam...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "slurmnode1: [2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "slurmnode1: [3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "slurmnode1: Loading extension module cpu_adam...\n",
      "slurmnode1: Time to load cpu_adam op: 21.546494960784912 seconds\n",
      "slurmnode2: Loading extension module cpu_adam...\n",
      "slurmnode2: Time to load cpu_adam op: 21.540149211883545 seconds\n",
      "slurmnode2: Loading extension module cpu_adam...\n",
      "slurmnode2: Time to load cpu_adam op: 21.474753618240356 seconds\n",
      "slurmnode1: Loading extension module cpu_adam...\n",
      "slurmnode1: Time to load cpu_adam op: 21.534514665603638 seconds\n",
      "slurmnode2: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode2: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode1: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: [2024-04-15 14:19:16,335] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode2: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode2: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: [2024-04-15 14:19:16,351] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "slurmnode1: [2024-04-15 14:19:16,351] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "slurmnode1: [2024-04-15 14:19:16,351] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "slurmnode1: [2024-04-15 14:19:16,351] [INFO] [engine.py:1410:_configure_zero_optimizer] Initializing ZeRO Stage 3\n",
      "slurmnode1: [2024-04-15 14:19:16,357] [INFO] [stage3.py:275:__init__] Reduce bucket size 500000000.0\n",
      "slurmnode1: [2024-04-15 14:19:16,357] [INFO] [stage3.py:276:__init__] Prefetch bucket size 500000000.0\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode2: Building extension module utils...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode1: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.053255558013916016 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.10198450088500977 seconds\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.10266232490539551 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.10173320770263672 seconds\n",
      "slurmnode1: [2024-04-15 14:19:17,058] [INFO] [stage3.py:567:_setup_for_real_optimizer] optimizer state initialized\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.0007305145263671875 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.0003466606140136719 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.0005197525024414062 seconds\n",
      "slurmnode1: [2024-04-15 14:19:19,197] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer\n",
      "slurmnode1: [2024-04-15 14:19:19,203] [INFO] [utils.py:829:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 1.03 GB         Max_CA 1 GB \n",
      "slurmnode1: [2024-04-15 14:19:19,204] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 26.36 GB, percent = 3.0%\n",
      "slurmnode1: [2024-04-15 14:19:19,204] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-04-15 14:19:19,204] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-04-15 14:19:19,204] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-04-15 14:19:19,204] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": true, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": true, \n",
      "slurmnode1:     \"cpu_checkpointing\": true, \n",
      "slurmnode1:     \"number_checkpoints\": 24, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": true, \n",
      "slurmnode1:     \"profile\": true\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:19:19,205] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 4\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   train_batch_size ............. 2048\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  128\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-04-15 14:19:19,206] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 3, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": true, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": {\n",
      "slurmnode1:         \"device\": \"cpu\", \n",
      "slurmnode1:         \"nvme_path\": null, \n",
      "slurmnode1:         \"buffer_count\": 5, \n",
      "slurmnode1:         \"buffer_size\": 1.000000e+08, \n",
      "slurmnode1:         \"max_in_cpu\": 1.000000e+09, \n",
      "slurmnode1:         \"pin_memory\": false\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"offload_optimizer\": {\n",
      "slurmnode1:         \"device\": \"cpu\", \n",
      "slurmnode1:         \"nvme_path\": null, \n",
      "slurmnode1:         \"buffer_count\": 4, \n",
      "slurmnode1:         \"pin_memory\": false, \n",
      "slurmnode1:         \"pipeline_read\": false, \n",
      "slurmnode1:         \"pipeline_write\": false, \n",
      "slurmnode1:         \"fast_init\": false, \n",
      "slurmnode1:         \"pipeline\": false\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1063:print]   zero_enabled ................. True\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 3\n",
      "slurmnode1: [2024-04-15 14:19:19,207] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 128, \n",
      "slurmnode1:     \"gradient_accumulation_steps\": 4, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.0003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0, \n",
      "slurmnode1:     \"activation_checkpointing\": {\n",
      "slurmnode1:         \"partition_activations\": true, \n",
      "slurmnode1:         \"cpu_checkpointing\": true, \n",
      "slurmnode1:         \"contiguous_memory_optimization\": true, \n",
      "slurmnode1:         \"number_checkpoints\": 24, \n",
      "slurmnode1:         \"synchronize_checkpoint_boundary\": true, \n",
      "slurmnode1:         \"profile\": true\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"fp16\": {\n",
      "slurmnode1:         \"enabled\": true\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"zero_optimization\": {\n",
      "slurmnode1:         \"stage\": 3, \n",
      "slurmnode1:         \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:         \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:         \"stage3_prefetch_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:         \"stage3_param_persitence_threshold\": 1.000000e+06, \n",
      "slurmnode1:         \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:         \"contiguous_gradients\": true, \n",
      "slurmnode1:         \"offload_optimizer\": {\n",
      "slurmnode1:             \"device\": \"cpu\"\n",
      "slurmnode1:         }, \n",
      "slurmnode1:         \"offload_param\": {\n",
      "slurmnode1:             \"device\": \"cpu\"\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.0004737377166748047 seconds\n",
      "slurmnode1: [2024-04-15 14:19:20,922] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information\n",
      "slurmnode1: [2024-04-15 14:19:20,923] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False\n",
      "slurmnode1: [2024-04-15 14:19:20,923] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with None total layers\n",
      "slurmnode1: [2024-04-15 14:19:20,923] [INFO] [checkpointing.py:554:forward] ----Synchronization False\n",
      "slurmnode1: [2024-04-15 14:19:20,923] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False\n",
      "slurmnode1: [2024-04-15 14:20:01,733] [INFO] [stage3.py:2281:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=4;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f07af14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: 04/15/2024 14:18:50 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode2: 04/15/2024 14:18:50 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode2: 04/15/2024 14:18:50 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode1: 04/15/2024 14:18:50 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode1: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode1: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode2: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode2: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode2: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 04/15/2024 14:18:52 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=4;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea9b94",
   "metadata": {},
   "source": [
    "Its really important that before you go to the next step you stop all of the executing and pending jobs or evaluation will faill!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47765d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./Assessment.ipynb\n",
      "./.gitignore\n",
      "./images/\n",
      "./images/cluster_overview.png\n",
      "./images/pipeline_parallel.png\n",
      "./images/MOE.png\n",
      "./images/1N_1gpu_utilization.png\n",
      "./images/tensorboard1.png\n",
      "./images/profiling_FP16_summary.png\n",
      "./images/profiling2.png\n",
      "./images/zero_memory_v2.png\n",
      "./images/profiling3.png\n",
      "./images/pipeline_parallel1.png\n",
      "./images/nvidia_smi.png\n",
      "./images/slurm_config.png\n",
      "./images/sinfo.jpg\n",
      "./images/nvlink_nvidia.png\n",
      "./images/profiling5_AMP.png\n",
      "./images/2N_4gpus_utilization.png\n",
      "./images/CIFAR-10.jpg\n",
      "./images/nvlink.png\n",
      "./images/profiling_FP16_checkpoiting_gradient_acc_memory.png\n",
      "./images/DLI_Header.png\n",
      "./images/profiling_FP16_checkpoiting_memory.png\n",
      "./images/nodes_communication.png\n",
      "./images/data_parallel.png\n",
      "./images/nvidiasmi.jpg\n",
      "./images/AMP.png\n",
      "./images/interactive_launch2.png\n",
      "./images/profiling1.png\n",
      "./images/activation_checkpoiting.png\n",
      "./images/SlurmConnect.jpg\n",
      "./images/interactive_launch1.png\n",
      "./images/CNN.png\n",
      "./images/tensor_parallel.jpg\n",
      "./images/profiling_hybrid.png\n",
      "./images/DLI Header.png\n",
      "./images/interactive_launch.png\n",
      "./images/deepspeed_MOE.png\n",
      "./images/Megatron_run.PNG\n",
      "./images/zero_memory.png\n",
      "./images/profiling4.png\n",
      "./images/1N_2gpus_utilization.png\n",
      "./images/profiling_FP16_memory.png\n",
      "./images/tensor_parallel1.png\n",
      "./images/tensor_parallel.png\n",
      "./images/nvlink_v2.png\n",
      "./images/interactive_launch0.png\n",
      "./images/megatron-lm.png\n",
      "./images/squeue.jpg\n",
      "./cifar-10-batches-py/\n",
      "./cifar-10-batches-py/data_batch_3\n",
      "./cifar-10-batches-py/batches.meta\n",
      "./cifar-10-batches-py/data_batch_1\n",
      "./cifar-10-batches-py/test_batch\n",
      "./cifar-10-batches-py/data_batch_4\n",
      "./cifar-10-batches-py/data_batch_2\n",
      "./cifar-10-batches-py/data_batch_5\n",
      "./cifar-10-batches-py/readme.html\n",
      "./.ipynb_checkpoints/\n",
      "./.ipynb_checkpoints/Assessment-checkpoint.ipynb\n",
      "./.dockerignore\n",
      "./minGPT/\n",
      "./minGPT/minGPT/\n",
      "./minGPT/minGPT/hostfile\n",
      "./minGPT/minGPT/README.md\n",
      "./minGPT/minGPT/play_math.ipynb\n",
      "./minGPT/minGPT/mingpt.jpg\n",
      "./minGPT/minGPT/runFirstDeepSpeed.py\n",
      "./minGPT/minGPT/LICENSE\n",
      "./minGPT/minGPT/.git/\n",
      "./minGPT/minGPT/.git/index\n",
      "./minGPT/minGPT/.git/logs/\n",
      "./minGPT/minGPT/.git/logs/refs/\n",
      "./minGPT/minGPT/.git/logs/refs/remotes/\n",
      "./minGPT/minGPT/.git/logs/refs/remotes/origin/\n",
      "./minGPT/minGPT/.git/logs/refs/remotes/origin/HEAD\n",
      "./minGPT/minGPT/.git/logs/refs/heads/\n",
      "./minGPT/minGPT/.git/logs/refs/heads/master\n",
      "./minGPT/minGPT/.git/logs/HEAD\n",
      "./minGPT/minGPT/.git/description\n",
      "./minGPT/minGPT/.git/refs/\n",
      "./minGPT/minGPT/.git/refs/remotes/\n",
      "./minGPT/minGPT/.git/refs/remotes/origin/\n",
      "./minGPT/minGPT/.git/refs/remotes/origin/HEAD\n",
      "./minGPT/minGPT/.git/refs/tags/\n",
      "./minGPT/minGPT/.git/refs/heads/\n",
      "./minGPT/minGPT/.git/refs/heads/master\n",
      "./minGPT/minGPT/.git/packed-refs\n",
      "./minGPT/minGPT/.git/config\n",
      "./minGPT/minGPT/.git/branches/\n",
      "./minGPT/minGPT/.git/hooks/\n",
      "./minGPT/minGPT/.git/hooks/post-update.sample\n",
      "./minGPT/minGPT/.git/hooks/applypatch-msg.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-commit.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-rebase.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-applypatch.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-merge-commit.sample\n",
      "./minGPT/minGPT/.git/hooks/update.sample\n",
      "./minGPT/minGPT/.git/hooks/commit-msg.sample\n",
      "./minGPT/minGPT/.git/hooks/prepare-commit-msg.sample\n",
      "./minGPT/minGPT/.git/hooks/fsmonitor-watchman.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-receive.sample\n",
      "./minGPT/minGPT/.git/hooks/pre-push.sample\n",
      "./minGPT/minGPT/.git/objects/\n",
      "./minGPT/minGPT/.git/objects/pack/\n",
      "./minGPT/minGPT/.git/objects/pack/pack-7d0002593b25497e94cad27bbf44ed91dc53eeb6.idx\n",
      "./minGPT/minGPT/.git/objects/pack/pack-7d0002593b25497e94cad27bbf44ed91dc53eeb6.pack\n",
      "./minGPT/minGPT/.git/objects/info/\n",
      "./minGPT/minGPT/.git/info/\n",
      "./minGPT/minGPT/.git/info/exclude\n",
      "./minGPT/minGPT/.git/HEAD\n",
      "./minGPT/minGPT/mingpt/\n",
      "./minGPT/minGPT/mingpt/trainer.py\n",
      "./minGPT/minGPT/mingpt/__init__.py\n",
      "./minGPT/minGPT/mingpt/__pycache__/\n",
      "./minGPT/minGPT/mingpt/__pycache__/__init__.cpython-38.pyc\n",
      "./minGPT/minGPT/mingpt/__pycache__/trainer.cpython-38.pyc\n",
      "./minGPT/minGPT/mingpt/__pycache__/model.cpython-38.pyc\n",
      "./minGPT/minGPT/mingpt/__pycache__/utils.cpython-38.pyc\n",
      "./minGPT/minGPT/mingpt/utils.py\n",
      "./minGPT/minGPT/mingpt/.ipynb_checkpoints/\n",
      "./minGPT/minGPT/mingpt/.ipynb_checkpoints/trainer-checkpoint.py\n",
      "./minGPT/minGPT/mingpt/.ipynb_checkpoints/model-checkpoint.py\n",
      "./minGPT/minGPT/mingpt/model.py\n",
      "./minGPT/minGPT/.gitignore\n",
      "./minGPT/minGPT/runSlurmStep3.sh\n",
      "./minGPT/minGPT/.ipynb_checkpoints/\n",
      "./minGPT/minGPT/.ipynb_checkpoints/runStartingPoint-checkpoint.py\n",
      "./minGPT/minGPT/.ipynb_checkpoints/runStep5-checkpoint.py\n",
      "./minGPT/minGPT/.ipynb_checkpoints/play_image-checkpoint.ipynb\n",
      "./minGPT/minGPT/.ipynb_checkpoints/runFirstDeepSpeed-checkpoint.py\n",
      "./minGPT/minGPT/play_char.ipynb\n",
      "./minGPT/minGPT/runSlurmStep5.sh\n",
      "./minGPT/minGPT/ds_config_step4.json\n",
      "./minGPT/minGPT/ds_config_basic.json\n",
      "./minGPT/minGPT/play_image.ipynb\n",
      "./minGPT/minGPT/runStartingPoint.py\n",
      "./minGPT/minGPT/runStep4.py\n",
      "./minGPT/minGPT/runSlurmStep4.sh\n",
      "./minGPT/minGPT/ds_config_step5.json\n",
      "./minGPT/minGPT/runStep5.py\n",
      "./data/\n",
      "./data/code/\n",
      "./data/code/pretrain_gpt_2Node4GPU.sh\n",
      "./data/code/test_2Node4GPU.sh\n",
      "./data/code/pretrain_gpt_1GPU.sh\n",
      "./data/code/pretrain_gpt_2GPU.sh\n",
      "./data/GPT-2_assets/\n",
      "./data/GPT-2_assets/my-gpt2_text_document.idx\n",
      "./data/GPT-2_assets/gpt2-merges.txt\n",
      "./data/GPT-2_assets/my-gpt2_text_document.bin\n",
      "./data/GPT-2_assets/gpt2-vocab.json\n",
      "./cuda-samples/\n",
      "./cuda-samples/bin/\n",
      "./cuda-samples/bin/x86_64/\n",
      "./cuda-samples/bin/x86_64/linux/\n",
      "./cuda-samples/bin/x86_64/linux/release/\n",
      "./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest\n",
      "./cuda-samples/bin/win64/\n",
      "./cuda-samples/bin/win64/Release/\n",
      "./cuda-samples/bin/win64/Release/glew64.dll\n",
      "./cuda-samples/bin/win64/Release/freeglut.dll\n",
      "./cuda-samples/bin/win64/Debug/\n",
      "./cuda-samples/bin/win64/Debug/glew64.dll\n",
      "./cuda-samples/bin/win64/Debug/freeglut.dll\n",
      "./cifar-10-python.tar.gz\n",
      "./megatron/\n",
      "./megatron/tensorboard/\n",
      "./megatron/tensorboard/.gitkeep\n",
      "./megatron/Megatron-LM/\n",
      "./megatron/Megatron-LM/README.md\n",
      "./megatron/Megatron-LM/LICENSE\n",
      "./megatron/Megatron-LM/pretrain_ict.py\n",
      "./megatron/Megatron-LM/.git/\n",
      "./megatron/Megatron-LM/.git/index\n",
      "./megatron/Megatron-LM/.git/logs/\n",
      "./megatron/Megatron-LM/.git/logs/refs/\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/origin/\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/origin/master\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/origin/main\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/origin/checkpoint_util\n",
      "./megatron/Megatron-LM/.git/logs/refs/remotes/origin/HEAD\n",
      "./megatron/Megatron-LM/.git/logs/refs/heads/\n",
      "./megatron/Megatron-LM/.git/logs/refs/heads/main\n",
      "./megatron/Megatron-LM/.git/logs/HEAD\n",
      "./megatron/Megatron-LM/.git/FETCH_HEAD\n",
      "./megatron/Megatron-LM/.git/description\n",
      "./megatron/Megatron-LM/.git/refs/\n",
      "./megatron/Megatron-LM/.git/refs/remotes/\n",
      "./megatron/Megatron-LM/.git/refs/remotes/origin/\n",
      "./megatron/Megatron-LM/.git/refs/remotes/origin/master\n",
      "./megatron/Megatron-LM/.git/refs/remotes/origin/main\n",
      "./megatron/Megatron-LM/.git/refs/remotes/origin/checkpoint_util\n",
      "./megatron/Megatron-LM/.git/refs/remotes/origin/HEAD\n",
      "./megatron/Megatron-LM/.git/refs/tags/\n",
      "./megatron/Megatron-LM/.git/refs/tags/v2.6\n",
      "./megatron/Megatron-LM/.git/refs/tags/v2.5\n",
      "./megatron/Megatron-LM/.git/refs/heads/\n",
      "./megatron/Megatron-LM/.git/refs/heads/main\n",
      "./megatron/Megatron-LM/.git/packed-refs\n",
      "./megatron/Megatron-LM/.git/config\n",
      "./megatron/Megatron-LM/.git/ORIG_HEAD\n",
      "./megatron/Megatron-LM/.git/branches/\n",
      "./megatron/Megatron-LM/.git/hooks/\n",
      "./megatron/Megatron-LM/.git/hooks/post-update.sample\n",
      "./megatron/Megatron-LM/.git/hooks/applypatch-msg.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-commit.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-rebase.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-applypatch.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-merge-commit.sample\n",
      "./megatron/Megatron-LM/.git/hooks/update.sample\n",
      "./megatron/Megatron-LM/.git/hooks/commit-msg.sample\n",
      "./megatron/Megatron-LM/.git/hooks/prepare-commit-msg.sample\n",
      "./megatron/Megatron-LM/.git/hooks/fsmonitor-watchman.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-receive.sample\n",
      "./megatron/Megatron-LM/.git/hooks/pre-push.sample\n",
      "./megatron/Megatron-LM/.git/objects/\n",
      "./megatron/Megatron-LM/.git/objects/pack/\n",
      "./megatron/Megatron-LM/.git/objects/pack/pack-7a28e23283026065305ec8fc70b9a528fbc88ae4.idx\n",
      "./megatron/Megatron-LM/.git/objects/pack/pack-7a28e23283026065305ec8fc70b9a528fbc88ae4.pack\n",
      "./megatron/Megatron-LM/.git/objects/pack/pack-cfa0f2be3fb265b1076148521b3ab4a83273e1f7.idx\n",
      "./megatron/Megatron-LM/.git/objects/pack/pack-cfa0f2be3fb265b1076148521b3ab4a83273e1f7.pack\n",
      "./megatron/Megatron-LM/.git/objects/info/\n",
      "./megatron/Megatron-LM/.git/info/\n",
      "./megatron/Megatron-LM/.git/info/exclude\n",
      "./megatron/Megatron-LM/.git/HEAD\n",
      "./megatron/Megatron-LM/pretrain_bert.py\n",
      "./megatron/Megatron-LM/megatron/\n",
      "./megatron/Megatron-LM/megatron/optimizer/\n",
      "./megatron/Megatron-LM/megatron/optimizer/__init__.py\n",
      "./megatron/Megatron-LM/megatron/optimizer/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/optimizer/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/optimizer/__pycache__/optimizer.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/optimizer/__pycache__/grad_scaler.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/optimizer/__pycache__/clip_grads.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/optimizer/optimizer.py\n",
      "./megatron/Megatron-LM/megatron/optimizer/clip_grads.py\n",
      "./megatron/Megatron-LM/megatron/optimizer/grad_scaler.py\n",
      "./megatron/Megatron-LM/megatron/model/\n",
      "./megatron/Megatron-LM/megatron/model/fused_layer_norm.py\n",
      "./megatron/Megatron-LM/megatron/model/module.py\n",
      "./megatron/Megatron-LM/megatron/model/__init__.py\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/transformer.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_bias_gelu.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/t5_model.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/enums.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/bert_model.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/module.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_bias_gelu.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/bert_model.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/language_model.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/transformer.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/enums.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_layer_norm.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/gpt_model.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/module.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/distributed.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_layer_norm.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/gpt_model.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/utils.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_softmax.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/fused_softmax.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/utils.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/language_model.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/t5_model.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/model/__pycache__/distributed.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/model/fused_softmax.py\n",
      "./megatron/Megatron-LM/megatron/model/biencoder_model.py\n",
      "./megatron/Megatron-LM/megatron/model/fused_bias_gelu.py\n",
      "./megatron/Megatron-LM/megatron/model/multiple_choice.py\n",
      "./megatron/Megatron-LM/megatron/model/transformer.py\n",
      "./megatron/Megatron-LM/megatron/model/utils.py\n",
      "./megatron/Megatron-LM/megatron/model/realm_model.py\n",
      "./megatron/Megatron-LM/megatron/model/bert_model.py\n",
      "./megatron/Megatron-LM/megatron/model/vision/\n",
      "./megatron/Megatron-LM/megatron/model/vision/vit_backbone.py\n",
      "./megatron/Megatron-LM/megatron/model/vision/classification.py\n",
      "./megatron/Megatron-LM/megatron/model/language_model.py\n",
      "./megatron/Megatron-LM/megatron/model/t5_model.py\n",
      "./megatron/Megatron-LM/megatron/model/gpt_model.py\n",
      "./megatron/Megatron-LM/megatron/model/distributed.py\n",
      "./megatron/Megatron-LM/megatron/model/enums.py\n",
      "./megatron/Megatron-LM/megatron/model/classification.py\n",
      "./megatron/Megatron-LM/megatron/mpu/\n",
      "./megatron/Megatron-LM/megatron/mpu/layers.py\n",
      "./megatron/Megatron-LM/megatron/mpu/__init__.py\n",
      "./megatron/Megatron-LM/megatron/mpu/random.py\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/random.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/data.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/random.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/mappings.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/mappings.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/cross_entropy.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/layers.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/layers.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/cross_entropy.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/data.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/utils.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/initialize.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/utils.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/__pycache__/initialize.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/mpu/utils.py\n",
      "./megatron/Megatron-LM/megatron/mpu/initialize.py\n",
      "./megatron/Megatron-LM/megatron/mpu/cross_entropy.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/test_layers.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/test_initialize.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/__init__.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/test_random.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/test_data.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/test_cross_entropy.py\n",
      "./megatron/Megatron-LM/megatron/mpu/tests/commons.py\n",
      "./megatron/Megatron-LM/megatron/mpu/mappings.py\n",
      "./megatron/Megatron-LM/megatron/mpu/data.py\n",
      "./megatron/Megatron-LM/megatron/global_vars.py\n",
      "./megatron/Megatron-LM/megatron/__init__.py\n",
      "./megatron/Megatron-LM/megatron/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/learning_rates.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/checkpointing.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/arguments.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/dist_signal_handler.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/microbatches.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/checkpointing.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/global_vars.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/memory.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/microbatches.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/arguments.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/global_vars.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/p2p_communication.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/memory.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/package_info.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/utils.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/training.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/package_info.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/initialize.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/training.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/utils.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/schedules.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/__pycache__/initialize.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/p2p_communication.py\n",
      "./megatron/Megatron-LM/megatron/training.py\n",
      "./megatron/Megatron-LM/megatron/dist_signal_handler.py\n",
      "./megatron/Megatron-LM/megatron/prof/\n",
      "./megatron/Megatron-LM/megatron/arguments.py\n",
      "./megatron/Megatron-LM/megatron/microbatches.py\n",
      "./megatron/Megatron-LM/megatron/fp16_deprecated/\n",
      "./megatron/Megatron-LM/megatron/fp16_deprecated/loss_scaler.py\n",
      "./megatron/Megatron-LM/megatron/utils.py\n",
      "./megatron/Megatron-LM/megatron/initialize.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/\n",
      "./megatron/Megatron-LM/megatron/text_generation/forward_step.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/__init__.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/generation.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/sampling.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/tokenization.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/api.py\n",
      "./megatron/Megatron-LM/megatron/text_generation/communication.py\n",
      "./megatron/Megatron-LM/megatron/memory.py\n",
      "./megatron/Megatron-LM/megatron/tokenizer/\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__init__.py\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/tokenizer.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/bert_tokenization.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/bert_tokenization.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/gpt2_tokenization.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/tokenizer.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/__pycache__/gpt2_tokenization.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/tokenizer/gpt2_tokenization.py\n",
      "./megatron/Megatron-LM/megatron/tokenizer/bert_tokenization.py\n",
      "./megatron/Megatron-LM/megatron/tokenizer/tokenizer.py\n",
      "./megatron/Megatron-LM/megatron/indexer.py\n",
      "./megatron/Megatron-LM/megatron/old_training.py\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/compat.h\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax_cuda.cu\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/__init__.py\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_softmax.cpp\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_softmax_cuda.cu\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax.cpp\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_softmax_cuda.cuda.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_masked_softmax.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_softmax.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_upper_triang_masked_softmax_cuda.cuda.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_masked_softmax_cuda.cuda.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/layer_norm_cuda.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_masked_softmax_cuda.so\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_upper_triang_masked_softmax_cuda.so\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_softmax_cuda.so\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/.ninja_log\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/fused_mix_prec_layer_norm_cuda.so\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/scaled_upper_triang_masked_softmax.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/.ninja_deps\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/build/layer_norm_cuda_kernel.cuda.o\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/tests/\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/tests/__init__.py\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/tests/test_fused_kernels.py\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax.h\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/layer_norm_cuda.cpp\n",
      "./megatron/Megatron-LM/megatron/fused_kernels/type_shim.h\n",
      "./megatron/Megatron-LM/megatron/data/\n",
      "./megatron/Megatron-LM/megatron/data/image_folder.py\n",
      "./megatron/Megatron-LM/megatron/data/ict_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/orqa_wiki_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/__init__.py\n",
      "./megatron/Megatron-LM/megatron/data/indexed_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/__init__.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/gpt_dataset.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/dataset_utils.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/__init__.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/gpt_dataset.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/data_samplers.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/indexed_dataset.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/indexed_dataset.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/dataset_utils.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/blendable_dataset.cpython-38.pyc\n",
      "./megatron/Megatron-LM/megatron/data/__pycache__/blendable_dataset.cpython-37.pyc\n",
      "./megatron/Megatron-LM/megatron/data/data_samplers.py\n",
      "./megatron/Megatron-LM/megatron/data/realm_index.py\n",
      "./megatron/Megatron-LM/megatron/data/gpt_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/vit_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/helpers.cpython-38-x86_64-linux-gnu.so\n",
      "./megatron/Megatron-LM/megatron/data/Makefile\n",
      "./megatron/Megatron-LM/megatron/data/autoaugment.py\n",
      "./megatron/Megatron-LM/megatron/data/realm_dataset_utils.py\n",
      "./megatron/Megatron-LM/megatron/data/test/\n",
      "./megatron/Megatron-LM/megatron/data/test/test_indexed_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/test/test_preprocess_data.sh\n",
      "./megatron/Megatron-LM/megatron/data/biencoder_dataset_utils.py\n",
      "./megatron/Megatron-LM/megatron/data/bert_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/helpers.cpp\n",
      "./megatron/Megatron-LM/megatron/data/dataset_utils.py\n",
      "./megatron/Megatron-LM/megatron/data/t5_dataset.py\n",
      "./megatron/Megatron-LM/megatron/data/blendable_dataset.py\n",
      "./megatron/Megatron-LM/megatron/learning_rates.py\n",
      "./megatron/Megatron-LM/megatron/schedules.py\n",
      "./megatron/Megatron-LM/megatron/checkpointing.py\n",
      "./megatron/Megatron-LM/megatron/text_generation_server.py\n",
      "./megatron/Megatron-LM/.gitlab-ci.yml\n",
      "./megatron/Megatron-LM/.gitignore\n",
      "./megatron/Megatron-LM/images/\n",
      "./megatron/Megatron-LM/images/cases_april2021.png\n",
      "./megatron/Megatron-LM/pretrain_t5.py\n",
      "./megatron/Megatron-LM/tests/\n",
      "./megatron/Megatron-LM/tests/test_basic.py\n",
      "./megatron/Megatron-LM/pretrain_gpt.py\n",
      "./megatron/Megatron-LM/examples/\n",
      "./megatron/Megatron-LM/examples/finetune_mnli_distributed.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_gpt_distributed.sh\n",
      "./megatron/Megatron-LM/examples/finetune_race_distributed.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_t5_distributed.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_gpt.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_ict.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_bert.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_bert_distributed_with_mp.sh\n",
      "./megatron/Megatron-LM/examples/evaluate_zeroshot_gpt.sh\n",
      "./megatron/Megatron-LM/examples/finetune_retriever_distributed.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_t5_distributed_with_mp.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_bert_distributed.sh\n",
      "./megatron/Megatron-LM/examples/merge_mp_bert.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_gpt3_175B.sh\n",
      "./megatron/Megatron-LM/examples/msdp/\n",
      "./megatron/Megatron-LM/examples/msdp/README.md\n",
      "./megatron/Megatron-LM/examples/msdp/data_processing.sh\n",
      "./megatron/Megatron-LM/examples/msdp/prompt_knwl_gen.sh\n",
      "./megatron/Megatron-LM/examples/msdp/eval_resp_generation.sh\n",
      "./megatron/Megatron-LM/examples/msdp/prompt_resp_gen.sh\n",
      "./megatron/Megatron-LM/examples/msdp/eval_knwl_generation.sh\n",
      "./megatron/Megatron-LM/examples/msdp/prep_resp_gen.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_gpt_distributed_with_mp.sh\n",
      "./megatron/Megatron-LM/examples/sc21/\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_12.sh\n",
      "./megatron/Megatron-LM/examples/sc21/SRUN.sh\n",
      "./megatron/Megatron-LM/examples/sc21/README.md\n",
      "./megatron/Megatron-LM/examples/sc21/SBATCH.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_16.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_13.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_11.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_18.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_15.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_17.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_figure_14.sh\n",
      "./megatron/Megatron-LM/examples/sc21/CONFIG.sh\n",
      "./megatron/Megatron-LM/examples/sc21/run_table_1.sh\n",
      "./megatron/Megatron-LM/examples/evaluate_retriever_nq.sh\n",
      "./megatron/Megatron-LM/examples/run_text_generation_server_345M_8_tensor_parallel.sh\n",
      "./megatron/Megatron-LM/examples/run_text_generation_server_345M.sh\n",
      "./megatron/Megatron-LM/examples/pretrain_t5.sh\n",
      "./megatron/Megatron-LM/tasks/\n",
      "./megatron/Megatron-LM/tasks/data_utils.py\n",
      "./megatron/Megatron-LM/tasks/zeroshot_gpt/\n",
      "./megatron/Megatron-LM/tasks/zeroshot_gpt/datasets.py\n",
      "./megatron/Megatron-LM/tasks/zeroshot_gpt/evaluate.py\n",
      "./megatron/Megatron-LM/tasks/zeroshot_gpt/detokenizer.py\n",
      "./megatron/Megatron-LM/tasks/glue/\n",
      "./megatron/Megatron-LM/tasks/glue/qqp.py\n",
      "./megatron/Megatron-LM/tasks/glue/mnli.py\n",
      "./megatron/Megatron-LM/tasks/glue/finetune.py\n",
      "./megatron/Megatron-LM/tasks/glue/data.py\n",
      "./megatron/Megatron-LM/tasks/eval_utils.py\n",
      "./megatron/Megatron-LM/tasks/race/\n",
      "./megatron/Megatron-LM/tasks/race/finetune.py\n",
      "./megatron/Megatron-LM/tasks/race/data.py\n",
      "./megatron/Megatron-LM/tasks/main.py\n",
      "./megatron/Megatron-LM/tasks/finetune_utils.py\n",
      "./megatron/Megatron-LM/tasks/vision/\n",
      "./megatron/Megatron-LM/tasks/vision/eval_utils.py\n",
      "./megatron/Megatron-LM/tasks/vision/main.py\n",
      "./megatron/Megatron-LM/tasks/vision/finetune_utils.py\n",
      "./megatron/Megatron-LM/tasks/vision/classification.py\n",
      "./megatron/Megatron-LM/tasks/ensemble_classifier.py\n",
      "./megatron/Megatron-LM/tasks/msdp/\n",
      "./megatron/Megatron-LM/tasks/msdp/README.md\n",
      "./megatron/Megatron-LM/tasks/msdp/main.py\n",
      "./megatron/Megatron-LM/tasks/msdp/preprocessing.py\n",
      "./megatron/Megatron-LM/tasks/msdp/evaluate.py\n",
      "./megatron/Megatron-LM/tasks/msdp/prompt.py\n",
      "./megatron/Megatron-LM/tasks/msdp/metrics.py\n",
      "./megatron/Megatron-LM/tasks/orqa/\n",
      "./megatron/Megatron-LM/tasks/orqa/README.md\n",
      "./megatron/Megatron-LM/tasks/orqa/unsupervised/\n",
      "./megatron/Megatron-LM/tasks/orqa/unsupervised/qa_utils.py\n",
      "./megatron/Megatron-LM/tasks/orqa/unsupervised/tokenizers.py\n",
      "./megatron/Megatron-LM/tasks/orqa/unsupervised/nq.py\n",
      "./megatron/Megatron-LM/tasks/orqa/evaluate_utils.py\n",
      "./megatron/Megatron-LM/tasks/orqa/supervised/\n",
      "./megatron/Megatron-LM/tasks/orqa/supervised/eval_utils.py\n",
      "./megatron/Megatron-LM/tasks/orqa/supervised/finetune.py\n",
      "./megatron/Megatron-LM/tasks/orqa/supervised/data.py\n",
      "./megatron/Megatron-LM/tasks/orqa/evaluate_orqa.py\n",
      "./megatron/Megatron-LM/tools/\n",
      "./megatron/Megatron-LM/tools/run_text_generation_server.py\n",
      "./megatron/Megatron-LM/tools/preprocess_data.py\n",
      "./megatron/Megatron-LM/tools/linter.py\n",
      "./megatron/Megatron-LM/tools/text_generation_cli.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/\n",
      "./megatron/Megatron-LM/tools/openwebtext/add_id.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/README.md\n",
      "./megatron/Megatron-LM/tools/openwebtext/remove_group_duplicates.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/find_duplicates.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/group_duplicate_url.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/cleanup_dataset.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/filter_ngrams.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/blacklist_urls.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/merge_jsons.py\n",
      "./megatron/Megatron-LM/tools/openwebtext/cleanup_fix_dataset.py\n",
      "./megatron/Megatron-LM/tools/merge_mp_partitions.py\n",
      "./megatron/Megatron-LM/pretrain_vit.py\n",
      "./megatron/logs/\n",
      "./megatron/logs/4.out\n",
      "./megatron/logs/3.err\n",
      "./megatron/logs/.gitkeep\n",
      "./megatron/logs/2.err\n",
      "./megatron/logs/2.out\n",
      "./megatron/logs/4.err\n",
      "./megatron/logs/3.out\n",
      "./megatron/checkpoints/\n",
      "./megatron/checkpoints/.gitkeep\n",
      "./code/\n",
      "./code/pretrain_gpt_2Node4GPU.sh\n",
      "./code/test.sh\n",
      "./code/pretrain_gpt_2Node4GPU_increase_BS.sh\n",
      "./code/moe/\n",
      "./code/moe/hostfile\n",
      "./code/moe/run_ds_2_node.sh\n",
      "./code/moe/ds_config_stage_3.json\n",
      "./code/moe/cifar10_non_deepspeed.py\n",
      "./code/moe/conf/\n",
      "./code/moe/conf/azure/\n",
      "./code/moe/conf/azure/ncv4/\n",
      "./code/moe/conf/azure/ncv4/graph.xml\n",
      "./code/moe/conf/azure/ncv4/topo.xml\n",
      "./code/moe/ds_config_stage_1.json:Zone.Identifier\n",
      "./code/moe/ds_config_stage_0.json\n",
      "./code/moe/ds_config_stage_3.json:Zone.Identifier\n",
      "./code/moe/large_model_deepspeed.py\n",
      "./code/moe/ds_config.json:Zone.Identifier\n",
      "./code/moe/ds_config_autotune.json\n",
      "./code/moe/.ipynb_checkpoints/\n",
      "./code/moe/.ipynb_checkpoints/ds_config_stage_3.json:Zone-checkpoint.Identifier\n",
      "./code/moe/.ipynb_checkpoints/ds_config_stage_3-checkpoint.json\n",
      "./code/moe/.ipynb_checkpoints/ds_config_stage_2-checkpoint.json\n",
      "./code/moe/.ipynb_checkpoints/ds_config_stage_0-checkpoint.json\n",
      "./code/moe/run_ds_moe.sh\n",
      "./code/moe/cifar10_deepspeed.py\n",
      "./code/moe/run_standalone.sh\n",
      "./code/moe/run_ds_autotune_2_node.sh\n",
      "./code/moe/cifar10_deepspeed_MOE.py\n",
      "./code/moe/ds_config_stage_1.json\n",
      "./code/moe/ds_config_stage_2.json:Zone.Identifier\n",
      "./code/moe/ds_config.json\n",
      "./code/moe/run_ds_moe_2_node.sh\n",
      "./code/moe/ds_config_stage_2.json\n",
      "./code/moe/cifar10_moe_deepspeed.py\n",
      "./code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n",
      "./code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpoiting.sh\n",
      "./code/test_sbatch.sbatch\n",
      "./code/pretrain_gpt_2Node4GPU_no_nvlink.sh\n",
      "./code/test_2Node4GPU.sh\n",
      "./code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
      "./code/pretrain_gpt_1GPU.sh\n",
      "./code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n",
      "./code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpoiting_gradient_accumulation.sh\n",
      "./code/test_2Node4GPU_solution.sh\n",
      "./code/pretrain_gpt_2Node4GPU_increase_BS_fp16.sh\n",
      "./code/pretrain_gpt_2GPU.sh\n",
      "tar: .: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude data.tar -czf data.tar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35c13b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52dbc1",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate\n",
    "\n",
    "If you have implemented all of the changes listed above, please provide the job ID verified in Step 5 in the code block below. If the challenges were completed correctly, an \"Assessment Passed!\" message will appear. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70fc6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Job ID: 4\n",
      "Assessment Passed! Congratulations!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "from run_assessment import run_assessment\n",
    "job_id = 4\n",
    "run_assessment(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802d57d",
   "metadata": {},
   "source": [
    "Once \"Assessment Passed!\" appears, please go back to the DLI portal and press the assess button. This will generate a certificate. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
